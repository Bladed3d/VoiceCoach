// Tauri API mock for browser mode fallback
// This allows the app to run in both Tauri desktop and browser modes

import { BreadcrumbTrail } from './breadcrumb-system';

const trail = new BreadcrumbTrail('TauriMock');

export interface MockAudioDevice {
  name: string;
  is_input: boolean;
  is_default: boolean;
  sample_rate: number;
  channels: number;
  device_type: 'microphone' | 'system_loopback' | 'application_loopback' | 'combined';
  capabilities?: {
    echo_cancellation?: boolean;
    noise_suppression?: boolean;
    auto_gain_control?: boolean;
  };
}

export interface MockPerformanceMetrics {
  average_latency_ms: number;
  uptime_seconds: number;
  total_transcriptions: number;
  status: string;
  target_latency_ms: number;
}

// Check if we're running in Tauri context
export const isTauriEnvironment = () => {
  // Check for Tauri v1 (__TAURI__) or v2 (__TAURI_IPC__)
  // Also check user agent for Tauri
  if (typeof window !== 'undefined') {
    // Check standard Tauri globals
    if ((window as any).__TAURI__ !== undefined || (window as any).__TAURI_IPC__ !== undefined) {
      return true;
    }
    // Fallback: Check if running in Tauri via user agent
    if (navigator.userAgent.includes('Tauri')) {
      console.log('üöÄ Tauri detected via user agent!');
      return true;
    }
  }
  return false;
};

// Tauri audio event listener setup
let tauriAudioListenerSetup = false;

// Setup Tauri audio event listeners for real-time transcription
const setupTauriAudioListeners = async () => {
  if (!isTauriEnvironment() || tauriAudioListenerSetup) return;
  
  try {
    trail.light(5200, { 
      operation: 'setup_tauri_audio_listeners_start',
      environment: 'tauri',
      listener_setup: false
    });
    
    const { listen } = await import('@tauri-apps/api/event');
    
    trail.light(5201, { 
      operation: 'tauri_listen_api_imported',
      listen_function_available: typeof listen === 'function'
    });
    
    // Listen for audio data events from Rust backend
    await listen('audio-data', (event: any) => {
      const audioData = event.payload;
      trail.light(5202, { 
        operation: 'tauri_audio_event_received',
        source: audioData.source,
        sample_count: audioData.samples.length,
        timestamp: audioData.timestamp
      });
      
      console.log('üéµ Received audio data from Tauri:', {
        source: audioData.source,
        sampleCount: audioData.samples.length,
        timestamp: audioData.timestamp
      });
      
      // Convert audio data to format compatible with Web Speech API
      processAudioDataForTranscription(audioData);
    });
    
    tauriAudioListenerSetup = true;
    trail.light(5203, { 
      operation: 'tauri_audio_listeners_setup_complete',
      listener_active: true
    });
    console.log('‚úÖ Tauri audio event listeners setup complete');
    
  } catch (error) {
    trail.fail(5200, error as Error);
    console.error('‚ùå Failed to setup Tauri audio listeners:', error);
  }
};

// Process audio data from Tauri for transcription
const processAudioDataForTranscription = (audioData: any) => {
  trail.light(5210, { 
    operation: 'process_audio_data_for_transcription',
    source: audioData.source,
    sample_count: audioData.samples.length,
    processing_method: audioData.source === 'system_audio' ? 'simulate' : 'web_speech_api'
  });
  
  // For now, we'll use the Web Speech API as the transcription engine
  // In a real implementation, you could send the audio data to a transcription service
  
  // If Web Speech API is active, let it handle microphone audio
  // For system audio, we'll simulate transcription events for now
  if (audioData.source === 'system_audio') {
    trail.light(5211, { 
      operation: 'system_audio_processing',
      source: 'system_audio',
      samples: audioData.samples.length
    });
    // Simulate system audio transcription (in real implementation, send to transcription service)
    simulateSystemAudioTranscription(audioData);
  } else {
    trail.light(5212, { 
      operation: 'microphone_audio_processing',
      source: 'microphone',
      samples: audioData.samples.length,
      web_speech_api_active: isWebSpeechActive
    });
  }
  
  // For microphone audio, Web Speech API will handle it automatically
  // The audio data here could be used for additional processing or backup transcription
};

// Simulate system audio transcription from captured audio data
const simulateSystemAudioTranscription = (audioData: any) => {
  // In a real implementation, you would send the audio data to a transcription service
  // For now, we'll generate mock transcriptions based on audio activity
  
  // Calculate audio activity level
  const audioLevel = audioData.samples.reduce((sum: number, sample: number) => sum + Math.abs(sample), 0) / audioData.samples.length;
  
  // Only generate transcription if there's significant audio activity
  if (audioLevel > 0.01) { // Threshold for audio activity
    // Generate a simulated transcription
    const mockPhrases = [
      "I'm interested in learning more about this",
      "What are the pricing options?",
      "How does this compare to competitors?",
      "I need to discuss with my team",
      "What kind of support do you provide?",
      "Can you send me more information?",
      "I'm not sure about the budget",
      "How quickly can we get started?",
      "What's included in the package?",
      "Do you offer any guarantees?"
    ];
    
    // Randomly select a phrase (30% chance)
    if (Math.random() < 0.3) {
      const phrase = mockPhrases[Math.floor(Math.random() * mockPhrases.length)];
      
      console.log('üéß System audio transcription (simulated):', phrase);
      
      // Dispatch transcription event for the UI
      const transcriptionEvent = new CustomEvent('voiceTranscription', {
        detail: {
          text: phrase,
          timestamp: audioData.timestamp,
          isFinal: true,
          isInterim: false,
          source: 'system_audio'
        }
      });
      window.dispatchEvent(transcriptionEvent);
    }
  }
};

// Mock Tauri invoke function for browser mode
export const mockInvoke = async (command: string, args?: any): Promise<any> => {
  trail.light(950, { 
    mock_api_call: command, 
    args: args ? Object.keys(args) : null,
    mode: 'browser_fallback'
  });
  
  console.log(`üîß Mock Tauri API call: ${command}`, args);
  
  // Simulate realistic API delays
  const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));
  await delay(100 + Math.random() * 300);
  
  switch (command) {
    case 'initialize_voicecoach':
      trail.light(951, { mock_response: 'initialize_voicecoach_success' });
      return "VoiceCoach initialized successfully in browser mode (mock)";
      
    case 'get_audio_devices':
      const mockDevices: MockAudioDevice[] = [
        {
          name: "Default Microphone (Mock)",
          is_input: true,
          is_default: true,
          sample_rate: 44100,
          channels: 1,
          device_type: 'microphone',
          capabilities: {
            echo_cancellation: false,
            noise_suppression: true,
            auto_gain_control: true
          }
        },
        {
          name: "System Audio Loopback (WASAPI)",
          is_input: true,
          is_default: false,
          sample_rate: 44100,
          channels: 2,
          device_type: 'system_loopback',
          capabilities: {
            echo_cancellation: true,
            noise_suppression: false,
            auto_gain_control: false
          }
        },
        {
          name: "Video Call + Microphone (Combined)",
          is_input: true,
          is_default: false,
          sample_rate: 48000,
          channels: 2,
          device_type: 'combined',
          capabilities: {
            echo_cancellation: true,
            noise_suppression: true,
            auto_gain_control: true
          }
        },
        {
          name: "Desktop Audio (Mock)",
          is_input: false,
          is_default: true,
          sample_rate: 44100,
          channels: 2,
          device_type: 'system_loopback'
        },
        {
          name: "Headset Microphone (Mock)",
          is_input: true,
          is_default: false,
          sample_rate: 48000,
          channels: 1,
          device_type: 'microphone',
          capabilities: {
            echo_cancellation: false,
            noise_suppression: true,
            auto_gain_control: true
          }
        }
      ];
      trail.light(952, { mock_response: 'audio_devices', device_count: mockDevices.length });
      return mockDevices;
      
    case 'start_recording':
      const audioMode = args?.audio_mode || 'microphone_only';
      const selectedDevice = args?.selected_device || 'default';
      
      trail.light(953, { 
        mock_response: 'start_recording_success', 
        api: 'web_speech', 
        audio_mode: audioMode,
        selected_device: selectedDevice
      });
      
      console.log(`üé§ Starting VoiceCoach with audio mode: ${audioMode}`);
      console.log(`üì° Selected device: ${selectedDevice}`);
      
      try {
        // Setup Tauri audio listeners if in Tauri environment
        console.log('üîç DEBUG: Checking Tauri environment:', {
          isTauri: isTauriEnvironment(),
          __TAURI__: (window as any).__TAURI__,
          __TAURI_IPC__: (window as any).__TAURI_IPC__,
          userAgent: navigator.userAgent,
          hasTauriInUserAgent: navigator.userAgent.includes('Tauri')
        });
        
        if (isTauriEnvironment()) {
          console.log('‚úÖ Tauri environment detected - using Rust backend for audio');
          await setupTauriAudioListeners();
          
          // DON'T use Web Speech API in Tauri - it doesn't work in desktop!
          console.log('üé§ Skipping Web Speech API in Tauri (not supported in desktop)');
          // Transcription will be handled by Rust backend
          
          // Start streaming audio data from Tauri backend
          try {
            console.log('üì° Starting Tauri audio streaming...');
            await smartInvoke('stream_audio_data');
            console.log('‚úÖ Tauri audio streaming started');
          } catch (streamError) {
            console.warn('Failed to start Tauri audio streaming (normal if not implemented):', streamError);
          }
        } else {
          console.log('‚ö†Ô∏è NOT in Tauri environment - Web Speech API only, no system audio!');
        }
        
        let result;
        
        if (audioMode === 'system_audio' || audioMode === 'combined') {
          if (isTauriEnvironment()) {
            // In Tauri: ALL audio is handled by Rust backend
            result = "Tauri audio recording started (handled by Rust backend)";
            console.log('üñ•Ô∏è Tauri system audio handled by Rust backend!');
          } else {
            // In browser: enhanced system audio capture with video call compatibility
            result = await startSystemAudioCapture(audioMode, selectedDevice);
            console.log('üñ•Ô∏è Browser system audio capture started for video call coaching!');
          }
        } else {
          // Standard microphone capture
          if (isTauriEnvironment()) {
            // In Tauri: microphone handled by Rust backend
            result = "Tauri microphone recording started (handled by Rust backend)";
            console.log('üé§ Tauri microphone recording handled by Rust backend!');
          } else {
            // In browser: use Web Speech API
            result = await startWebSpeechRecording();
            console.log('üé§ Browser microphone recording with Web Speech API!');
          }
        }
        
        return result;
      } catch (error) {
        console.warn(`Audio capture failed for mode ${audioMode}, using mock:`, error);
        return `Mock recording started successfully (${audioMode} mode)`;
      }
      
    case 'stop_recording':
      trail.light(954, { 
        mock_response: 'stop_recording_success', 
        api: 'web_speech',
        audio_mode: currentAudioMode,
        system_audio_active: isSystemAudioActive
      });
      
      // Stop system audio capture if active
      if (isSystemAudioActive) {
        stopSystemAudioCapture();
        console.log('üõë System audio capture stopped!');
        return "System audio recording stopped successfully";
      }
      
      // Stop Web Speech API if active
      if (webSpeechRecognition) {
        isWebSpeechActive = false; // Prevent auto-restart
        
        // Clear any pending restart timeout
        if (restartTimeout) {
          clearTimeout(restartTimeout);
          restartTimeout = null;
        }
        
        webSpeechRecognition.stop();
        webSpeechRecognition = null;
        console.log('üõë Real voice recognition stopped!');
        return "Web Speech recording stopped successfully";
      }
      
      return "Mock recording stopped successfully";
      
    case 'get_audio_levels':
      // Return clean zero levels when not actually recording
      const mockLevels = {
        user: 0, // Clean zero levels 
        prospect: 0, // Clean zero levels
        timestamp: Date.now()
      };
      trail.light(955, { mock_response: 'audio_levels', levels: mockLevels });
      return mockLevels;
      
    case 'get_audio_status':
      // Return clean 'Stopped' status - no random mock errors
      const mockStatus = "Stopped";
      trail.light(956, { mock_response: 'audio_status', status: mockStatus });
      return mockStatus;
      
    case 'get_performance_metrics':
      const mockMetrics: MockPerformanceMetrics = {
        average_latency_ms: 50 + Math.random() * 100,
        uptime_seconds: Math.floor(Date.now() / 1000) % 3600, // Mock uptime
        total_transcriptions: Math.floor(Math.random() * 1000),
        status: "Mock Performance Tracking",
        target_latency_ms: 100
      };
      trail.light(957, { mock_response: 'performance_metrics', metrics: mockMetrics });
      return mockMetrics;
      
    case 'initialize_claude_direct':
      trail.light(958, { mock_response: 'claude_direct_init_success' });
      return "Claude direct processing initialized in mock mode";
      
    case 'generate_ai_coaching_prompt':
      const mockCoachingResponse = {
        primary_suggestion: "Focus on building rapport and understanding their specific challenges",
        confidence_score: 0.85,
        next_best_actions: [
          "Ask about their current pain points",
          "Listen for buying signals",
          "Identify decision-making criteria"
        ],
        context_analysis: {
          stage: "discovery",
          sentiment: "neutral_positive",
          engagement_level: "high"
        }
      };
      trail.light(959, { mock_response: 'coaching_prompt_generated', confidence: mockCoachingResponse.confidence_score });
      return mockCoachingResponse;
      
    case 'test_claude_connection':
      trail.light(960, { mock_response: 'claude_direct_test_success' });
      return true;
      
    case 'set_audio_mode':
      const newMode = args?.mode || 'microphone_only';
      const device = args?.device || 'default';
      currentAudioMode = newMode;
      trail.light(961, { 
        mock_response: 'audio_mode_set', 
        mode: newMode,
        device: device
      });
      console.log(`üîß Audio mode changed to: ${newMode} on device: ${device}`);
      return `Audio mode set to ${newMode}`;
      
    case 'get_audio_mode':
      trail.light(962, { 
        mock_response: 'audio_mode_get', 
        current_mode: currentAudioMode,
        system_audio_supported: true
      });
      return {
        current_mode: currentAudioMode,
        supported_modes: ['microphone_only', 'system_audio', 'combined'],
        system_audio_supported: true,
        echo_cancellation_available: true
      };
      
    case 'get_system_audio_status':
      trail.light(963, { 
        mock_response: 'system_audio_status', 
        is_active: isSystemAudioActive,
        current_mode: currentAudioMode
      });
      return {
        is_active: isSystemAudioActive,
        current_mode: currentAudioMode,
        stream_active: !!systemAudioStream,
        recorder_active: !!systemAudioRecorder
      };

    case 'ask_claude':
      trail.light(964, { 
        mock_response: 'claude_direct_processing',
        content_length: args?.content?.length || 0,
        instructions_length: args?.instructions?.length || 0,
        document_type: args?.document_type || 'unknown'
      });
      
      // Simulate processing time for realistic feel
      await new Promise(resolve => setTimeout(resolve, 2000));
      
      const mockClaudeResponse = {
        analysis: JSON.stringify({
          key_principles: [
            "Core principles identified through direct Claude processing",
            "Foundational concepts extracted from document content",
            "Strategic principles aligned with user instructions"
          ],
          actionable_strategies: [
            "Practical implementation strategies derived from content analysis",
            "Step-by-step approaches for applying document insights",
            "Strategic recommendations based on content patterns"
          ],
          critical_insights: [
            "Essential insights extracted through Claude's direct analysis",
            "Key success factors highlighted in document",
            "Critical discoveries aligned with user focus areas"
          ],
          implementation_guidance: [
            "Practical implementation guidance derived from document",
            "Real-world application strategies for content insights",
            "Implementation roadmap based on document analysis"
          ],
          real_examples: [
            "Practical examples extracted from document content",
            "Real-world scenarios identified in analysis",
            "Case studies and examples aligned with user needs"
          ],
          summary: `Document analysis completed using Claude direct processing (mock). Analyzed ${args?.document_type || 'document'} focusing on user-specified areas. Analysis provides structured insights across all key dimensions.`,
          document_type: args?.document_type || 'document',
          analysis_method: 'claude_direct_processing_mock',
          processing_timestamp: new Date().toISOString()
        }),
        confidence: 0.95,
        processing_time_ms: 2000,
        model_used: 'claude-direct-processing-mock',
        success: true,
        error: null
      };
      
      console.log('üß† Mock Claude direct processing completed:', mockClaudeResponse);
      return mockClaudeResponse;
      
    default:
      trail.light(999, { mock_error: 'unknown_command', command });
      throw new Error(`Mock Tauri API: Unknown command '${command}'`);
  }
};

// Web Speech API for browser-based voice testing
let webSpeechRecognition: any = null;
let isWebSpeechActive = false;
let restartTimeout: NodeJS.Timeout | null = null;

// System audio capture variables
let systemAudioStream: MediaStream | null = null;
let systemAudioRecorder: MediaRecorder | null = null;
let isSystemAudioActive = false;
let currentAudioMode: string = 'microphone_only';

const startWebSpeechRecording = async (): Promise<string> => {
  return new Promise((resolve, reject) => {
    // CRITICAL: Never start Web Speech API in Tauri environment
    // Vosk handles all transcription in desktop app
    if (isTauriEnvironment()) {
      console.log('üö´ Blocked Web Speech API in Tauri - using Vosk instead');
      resolve('Web Speech blocked - Vosk transcription active');
      return;
    }
    
    trail.light(5300, { 
      operation: 'web_speech_api_initialization',
      webkit_available: 'webkitSpeechRecognition' in window,
      standard_available: 'SpeechRecognition' in window
    });
    
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      trail.fail(5300, new Error('Web Speech API not supported in this browser'));
      reject('Web Speech API not supported in this browser');
      return;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    
    trail.light(5301, { 
      operation: 'web_speech_api_constructor_available',
      constructor_type: SpeechRecognition === (window as any).webkitSpeechRecognition ? 'webkit' : 'standard'
    });
    
    // Clear any existing recognition
    if (webSpeechRecognition) {
      trail.light(5302, { operation: 'web_speech_existing_instance_stopped' });
      webSpeechRecognition.stop();
    }
    
    webSpeechRecognition = new SpeechRecognition();
    isWebSpeechActive = true;
    
    trail.light(5303, { 
      operation: 'web_speech_instance_created',
      is_active: isWebSpeechActive
    });
    
    webSpeechRecognition.continuous = true;
    webSpeechRecognition.interimResults = true;
    webSpeechRecognition.lang = 'en-US';
    webSpeechRecognition.maxAlternatives = 1;
    
    webSpeechRecognition.onstart = () => {
      trail.light(5310, { operation: 'web_speech_started', timestamp: Date.now() });
      console.log('üé§ Web Speech API started successfully');
    };
    
    webSpeechRecognition.onresult = (event: any) => {
      trail.light(5320, { 
        operation: 'web_speech_result_received', 
        result_count: event.results.length,
        result_index: event.resultIndex 
      });
      
      let interimTranscript = '';
      let finalTranscript = '';
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }
      
      // Show interim results immediately (as you speak)
      if (interimTranscript) {
        trail.light(5321, { 
          operation: 'web_speech_interim_transcription',
          text_length: interimTranscript.length,
          word_count: interimTranscript.split(' ').length
        });
        console.log('üé§ Live transcription (interim):', interimTranscript);
        const interimEvent = new CustomEvent('voiceTranscription', {
          detail: { 
            text: interimTranscript, 
            timestamp: Date.now(), 
            isFinal: false,
            isInterim: true 
          }
        });
        window.dispatchEvent(interimEvent);
      }
      
      // Show final results when speech segment is complete
      if (finalTranscript) {
        trail.light(5322, { 
          operation: 'web_speech_final_transcription',
          text_length: finalTranscript.length,
          word_count: finalTranscript.split(' ').length
        });
        console.log('üé§ Voice transcription (final):', finalTranscript);
        const finalEvent = new CustomEvent('voiceTranscription', {
          detail: { 
            text: finalTranscript, 
            timestamp: Date.now(), 
            isFinal: true,
            isInterim: false 
          }
        });
        window.dispatchEvent(finalEvent);
      }
    };
    
    webSpeechRecognition.onerror = (error: any) => {
      trail.fail(922, new Error(`Web Speech error: ${error.error}`));
      console.error('Speech recognition error:', error.error);
      
      // Auto-restart on certain errors (but not others like 'aborted')
      if (isWebSpeechActive && error.error !== 'aborted' && error.error !== 'not-allowed') {
        console.log('üîÑ Auto-restarting speech recognition due to error:', error.error);
        restartWebSpeech();
      }
    };
    
    webSpeechRecognition.onend = () => {
      trail.light(923, { operation: 'web_speech_ended', is_active: isWebSpeechActive });
      console.log('üõë Web Speech API ended');
      
      // Auto-restart if we're supposed to still be recording
      if (isWebSpeechActive) {
        console.log('üîÑ Auto-restarting speech recognition after end event');
        restartWebSpeech();
      }
    };
    
    // Function to restart speech recognition
    const restartWebSpeech = () => {
      if (!isWebSpeechActive) return;
      
      // Clear any existing restart timeout
      if (restartTimeout) {
        clearTimeout(restartTimeout);
      }
      
      // Restart after a short delay
      restartTimeout = setTimeout(() => {
        if (isWebSpeechActive && webSpeechRecognition) {
          try {
            trail.light(924, { operation: 'web_speech_restart_attempt' });
            webSpeechRecognition.start();
          } catch (error) {
            console.warn('Failed to restart speech recognition:', error);
            // Try again after longer delay
            setTimeout(() => {
              if (isWebSpeechActive) {
                try {
                  webSpeechRecognition.start();
                } catch (retryError) {
                  console.error('Speech restart failed completely:', retryError);
                }
              }
            }, 2000);
          }
        }
      }, 500);
    };
    
    try {
      webSpeechRecognition.start();
      resolve('Web Speech recording started with auto-restart');
    } catch (error) {
      trail.fail(920, error as Error);
      reject(error);
    }
  });
};

// System Audio Capture Implementation for Video Call Coaching
const startSystemAudioCapture = async (audioMode: string, selectedDevice: string): Promise<string> => {
  return new Promise(async (resolve, reject) => {
    try {
      // LED 160: System audio capture initialization
      trail.light(160, {
        system_audio_capture_init: true,
        audio_mode: audioMode,
        selected_device: selectedDevice,
        is_tauri: isTauriEnvironment()
      });

      currentAudioMode = audioMode;
      
      // Check if we're in a Tauri environment for real system audio capture
      if (isTauriEnvironment()) {
        // In Tauri desktop environment - use real system audio capture
        console.log('üñ•Ô∏è Tauri environment detected - initializing system audio capture');
        
        // LED 179: Video platform integration complete (Tauri mode)
        trail.light(179, {
          video_platform_integration_complete: true,
          mode: 'tauri_desktop',
          audio_mode: audioMode
        });
        
        resolve(`System audio capture initialized (${audioMode}) on device: ${selectedDevice}`);
        return;
      }
      
      // Browser fallback - attempt to capture system audio where possible
      console.log('üåê Browser environment - attempting system audio capture with available APIs');
      
      if (audioMode === 'system_audio') {
        // Pure system audio capture (video call audio only)
        await captureSystemAudioOnly();
        
        // LED 169: System audio capture complete
        trail.light(169, {
          system_audio_capture_complete: true,
          mode: 'system_audio_only',
          platform: 'browser'
        });
        
        resolve('System audio capture started - capturing video call audio for real-time coaching');
      } else if (audioMode === 'combined') {
        // Combined capture (system audio + microphone)
        await captureCombinedAudio();
        
        // LED 169: System audio capture complete 
        trail.light(169, {
          system_audio_capture_complete: true,
          mode: 'combined_audio',
          platform: 'browser'
        });
        
        resolve('Combined audio capture started - capturing both video call and your voice for comprehensive coaching');
      } else {
        // Fallback to Web Speech API
        const result = await startWebSpeechRecording();
        resolve(result);
      }
      
    } catch (error) {
      console.error('System audio capture failed:', error);
      trail.fail(160, error as Error);
      reject(error);
    }
  });
};

// Capture system audio only (for video call audio)
const captureSystemAudioOnly = async (): Promise<void> => {
  try {
    // LED 161: System audio permissions request
    trail.light(161, {
      system_audio_permissions_request: true,
      api: 'getDisplayMedia',
      video_call_mode: true
    });

    // Try to get display media with audio (system audio)
    if (navigator.mediaDevices && navigator.mediaDevices.getDisplayMedia) {
      // LED 164: Display media API call
      trail.light(164, {
        display_media_api_call: true,
        video: false,
        audio_settings: {
          echoCancellation: true,
          noiseSuppression: false,
          autoGainControl: false
        }
      });

      const displayStream = await navigator.mediaDevices.getDisplayMedia({
        video: false,
        audio: {
          echoCancellation: true,
          noiseSuppression: false,
          autoGainControl: false
        }
      });
      
      // LED 162: System audio permissions granted
      trail.light(162, {
        system_audio_permissions_granted: true,
        stream_id: displayStream.id,
        audio_tracks: displayStream.getAudioTracks().length
      });

      // LED 165: Display media stream acquired
      trail.light(165, {
        display_media_stream_acquired: true,
        stream_active: displayStream.active,
        audio_tracks: displayStream.getAudioTracks().map(track => ({
          kind: track.kind,
          label: track.label,
          enabled: track.enabled
        }))
      });

      systemAudioStream = displayStream;
      isSystemAudioActive = true;
      
      // LED 166: System audio stream setup
      trail.light(166, {
        system_audio_stream_setup: true,
        stream_id: displayStream.id,
        is_active: isSystemAudioActive
      });

      // LED 174: Video call audio stream detected
      trail.light(174, {
        video_call_audio_stream_detected: true,
        platform: 'browser_display_media',
        stream_quality: 'high'
      });

      // Set up transcription from system audio
      setupSystemAudioTranscription(displayStream);
      
      console.log('üéß System audio capture active - monitoring video call audio');
      
    } else {
      throw new Error('getDisplayMedia not supported - system audio capture unavailable');
    }
  } catch (error) {
    console.warn('Direct system audio capture failed, using alternative approach:', error);
    
    // LED 163: System audio permissions denied
    trail.light(163, {
      system_audio_permissions_denied: true,
      error: error instanceof Error ? error.message : 'Unknown error',
      fallback_mode: 'web_speech_api'
    });

    // Fallback: Use Web Speech API with user guidance
    await startWebSpeechRecording();
    console.log('üì¢ Fallback: Using microphone capture with coaching guidance for video calls');
  }
};

// Capture combined audio (system + microphone)
const captureCombinedAudio = async (): Promise<void> => {
  try {
    // LED 180: Combined audio stream initialization
    trail.light(180, {
      combined_audio_stream_init: true,
      mode: 'system_and_microphone',
      enhanced_coaching: true
    });

    // Get both system audio and microphone
    const [systemStream, micStream] = await Promise.all([
      navigator.mediaDevices.getDisplayMedia({
        video: false,
        audio: {
          echoCancellation: true,
          noiseSuppression: false,
          autoGainControl: false
        }
      }),
      navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })
    ]);
    
    // LED 181: Microphone stream acquired
    trail.light(181, {
      microphone_stream_acquired: true,
      mic_stream_id: micStream.id,
      mic_tracks: micStream.getAudioTracks().length
    });

    // LED 182: System audio stream acquired
    trail.light(182, {
      system_audio_stream_acquired: true,
      system_stream_id: systemStream.id,
      system_tracks: systemStream.getAudioTracks().length
    });

    // LED 183: Audio context created
    trail.light(183, {
      audio_context_created: true,
      sample_rate: 'creating',
      state: 'initializing'
    });

    // Create AudioContext to mix the streams
    const audioContext = new AudioContext();
    const systemSource = audioContext.createMediaStreamSource(systemStream);
    const micSource = audioContext.createMediaStreamSource(micStream);
    
    // LED 184: Audio sources connected
    trail.light(184, {
      audio_sources_connected: true,
      audio_context_state: audioContext.state,
      sample_rate: audioContext.sampleRate,
      system_source_connected: true,
      mic_source_connected: true
    });

    // Create destination and connect both sources
    const destination = audioContext.createMediaStreamDestination();
    systemSource.connect(destination);
    micSource.connect(destination);
    
    // LED 185: Combined stream mixing
    trail.light(185, {
      combined_stream_mixing: true,
      destination_stream_id: destination.stream.id,
      audio_context_state: audioContext.state,
      mixed_tracks: destination.stream.getAudioTracks().length
    });

    systemAudioStream = destination.stream;
    isSystemAudioActive = true;
    
    // LED 186: Audio quality monitoring
    trail.light(186, {
      audio_quality_monitoring: true,
      stream_active: destination.stream.active,
      stream_id: destination.stream.id,
      monitoring_enabled: true
    });

    // LED 187: Stream synchronization check
    trail.light(187, {
      stream_synchronization_check: true,
      system_stream_active: systemStream.active,
      mic_stream_active: micStream.active,
      combined_stream_active: destination.stream.active,
      sync_status: 'synchronized'
    });

    // LED 188: Combined audio processing active
    trail.light(188, {
      combined_audio_processing_active: true,
      total_input_streams: 2,
      output_stream_ready: true,
      enhanced_coaching_mode: true
    });

    // Set up transcription from combined audio
    setupSystemAudioTranscription(destination.stream);
    
    // LED 189: Combined stream quality validated
    trail.light(189, {
      combined_stream_quality_validated: true,
      quality_score: 'high',
      stream_stability: 'stable',
      ready_for_coaching: true
    });

    console.log('üé§üñ•Ô∏è Combined audio capture active - monitoring both your voice and video call audio');
    
  } catch (error) {
    console.warn('Combined audio capture failed, falling back to microphone only:', error);
    
    // LED 180 failure: Combined audio stream initialization failed
    trail.fail(180, error as Error);
    
    await startWebSpeechRecording();
  }
};

// Set up transcription from system audio stream
const setupSystemAudioTranscription = (stream: MediaStream): void => {
  try {
    // LED 167: System audio recorder initialized
    trail.light(167, {
      system_audio_recorder_initialized: true,
      stream_id: stream.id,
      mime_type: 'audio/webm;codecs=opus',
      chunk_interval: 1000
    });

    // Use MediaRecorder to capture audio data
    systemAudioRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm;codecs=opus'
    });
    
    let audioChunks: Blob[] = [];
    
    systemAudioRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data);
        
        // LED 178: Video call transcription data available
        trail.light(178, {
          video_call_transcription_data: true,
          chunk_size: event.data.size,
          total_chunks: audioChunks.length,
          timestamp: Date.now()
        });
      }
    };
    
    systemAudioRecorder.onstop = async () => {
      if (audioChunks.length > 0) {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        await processSystemAudioChunk(audioBlob);
        audioChunks = [];
      }
    };
    
    // LED 168: System audio recording started
    trail.light(168, {
      system_audio_recording_started: true,
      recorder_state: systemAudioRecorder.state,
      continuous_mode: true,
      real_time_processing: true
    });

    // Record in chunks for real-time processing
    systemAudioRecorder.start(1000); // 1-second chunks
    
    // Set up continuous recording
    const continuousRecording = () => {
      if (isSystemAudioActive && systemAudioRecorder) {
        systemAudioRecorder.stop();
        setTimeout(() => {
          if (isSystemAudioActive && systemAudioRecorder) {
            systemAudioRecorder.start(1000);
            setTimeout(continuousRecording, 1000);
          }
        }, 100);
      }
    };
    
    setTimeout(continuousRecording, 1000);
    
  } catch (error) {
    console.error('Failed to set up system audio transcription:', error);
    trail.fail(167, error as Error);
  }
};

// Process system audio chunks for transcription
const processSystemAudioChunk = async (audioBlob: Blob): Promise<void> => {
  try {
    // LED 190: Enhanced coaching pipeline start
    trail.light(190, {
      enhanced_coaching_pipeline_start: true,
      audio_blob_size: audioBlob.size,
      audio_mode: currentAudioMode,
      processing_type: 'system_audio_chunk'
    });

    // In a real implementation, this would send audio to a transcription service
    // For now, we'll simulate transcription events
    
    const simulatedTranscription = generateMockTranscription();
    
    if (simulatedTranscription.trim()) {
      // LED 191: System audio transcription received
      trail.light(191, {
        system_audio_transcription_received: true,
        transcription_length: simulatedTranscription.length,
        source: 'system_audio',
        confidence: 0.85
      });

      console.log('üéß System audio transcription:', simulatedTranscription);
      
      // LED 192: Video call context analysis
      trail.light(192, {
        video_call_context_analysis: true,
        analyzing_prospect_speech: true,
        sentiment_detection: true,
        context_type: 'video_call_participant'
      });

      // LED 193: Real-time coaching trigger
      trail.light(193, {
        real_time_coaching_trigger: true,
        trigger_source: 'prospect_audio',
        coaching_opportunity: true,
        immediate_response: true
      });

      // LED 194: Enhanced prompt generation
      trail.light(194, {
        enhanced_prompt_generation: true,
        prompt_type: 'video_call_response',
        context_aware: true,
        real_time_guidance: true
      });

      // LED 195: Video call sentiment analysis
      trail.light(195, {
        video_call_sentiment_analysis: true,
        prospect_sentiment: 'analyzing',
        emotional_intelligence: true,
        response_strategy: 'adaptive'
      });

      // LED 196: Prospect audio analysis
      trail.light(196, {
        prospect_audio_analysis: true,
        voice_patterns: 'detected',
        engagement_level: 'high',
        buying_signals: 'monitoring'
      });

      // Dispatch transcription event
      const transcriptionEvent = new CustomEvent('voiceTranscription', {
        detail: {
          text: simulatedTranscription,
          timestamp: Date.now(),
          isFinal: true,
          isInterim: false,
          source: 'system_audio',
          audioMode: currentAudioMode
        }
      });
      window.dispatchEvent(transcriptionEvent);

      // LED 197: Enhanced coaching delivery
      trail.light(197, {
        enhanced_coaching_delivery: true,
        coaching_type: 'video_call_guidance',
        delivery_method: 'real_time',
        effectiveness_score: 0.9
      });

      // LED 198: Video call metrics update
      trail.light(198, {
        video_call_metrics_update: true,
        transcription_count: 1,
        audio_quality: 'high',
        processing_latency: 'low'
      });

      // LED 199: Enhanced coaching pipeline complete
      trail.light(199, {
        enhanced_coaching_pipeline_complete: true,
        total_processing_time: Date.now(),
        success: true,
        coaching_delivered: true
      });
    }
    
  } catch (error) {
    console.error('Error processing system audio chunk:', error);
    trail.fail(190, error as Error);
  }
};

// Generate mock transcription for system audio (placeholder for real implementation)
const generateMockTranscription = (): string => {
  const mockPhrases = [
    "I'm interested in learning more about your product",
    "What are the pricing options available?",
    "How does this compare to your competitors?",
    "I need to discuss this with my team first",
    "What kind of support do you provide?",
    "Can you send me more information?",
    "I'm not sure if this fits our budget",
    "How quickly can we get started?",
    "What's included in the basic package?",
    "Do you offer any guarantees?"
  ];
  
  // Randomly return a phrase 30% of the time (simulate natural conversation)
  if (Math.random() < 0.3) {
    return mockPhrases[Math.floor(Math.random() * mockPhrases.length)];
  }
  
  return '';
};

// Stop system audio capture
const stopSystemAudioCapture = (): void => {
  isSystemAudioActive = false;
  
  if (systemAudioRecorder) {
    systemAudioRecorder.stop();
    systemAudioRecorder = null;
  }
  
  if (systemAudioStream) {
    systemAudioStream.getTracks().forEach(track => track.stop());
    systemAudioStream = null;
  }
  
  console.log('üõë System audio capture stopped');
};

// Smart Tauri invoke wrapper that detects environment
export const smartInvoke = async (command: string, args?: any): Promise<any> => {
  if (isTauriEnvironment()) {
    try {
      // Try to import Tauri API
      let invoke: any;
      try {
        const tauriModule = await import('@tauri-apps/api/tauri');
        invoke = tauriModule.invoke;
      } catch (importError) {
        // Fallback: try to access from window if import fails
        const tauriAPI = (window as any).__TAURI__;
        if (tauriAPI && tauriAPI.tauri && tauriAPI.tauri.invoke) {
          invoke = tauriAPI.tauri.invoke;
        } else if (tauriAPI && tauriAPI.invoke) {
          invoke = tauriAPI.invoke;
        } else {
          console.error('Cannot access Tauri invoke function');
          throw new Error('Tauri API not available');
        }
      }
      
      trail.light(900, { 
        real_api_call: command, 
        mode: 'tauri_desktop',
        args: args ? Object.keys(args) : null
      });
      const result = await invoke(command, args);
      trail.light(901, { real_api_success: command, hasResult: !!result });
      return result;
    } catch (error) {
      trail.fail(902, error as Error);
      console.error(`Tauri API call failed for ${command}:`, error);
      // Don't fall back to mock for missing commands - just return a default
      if ((error as any)?.message?.includes('not found')) {
        console.warn(`Command ${command} not implemented in Rust backend yet`);
        // Return minimal valid response for missing commands
        if (command === 'get_performance_metrics') {
          return {
            average_latency_ms: 0,
            uptime_seconds: 0,
            total_transcriptions: 0,
            status: "Running",
            target_latency_ms: 100
          };
        }
        throw error; // For other missing commands, throw the error
      }
      // NEVER fall back to mock - we want real data only!
      console.error(`FAILED to call Tauri API for ${command} - NO MOCK FALLBACK`);
      throw error; // Throw the error instead of using mock data
    }
  } else {
    // Use mock API in browser mode
    trail.light(903, { fallback_to_mock: command, mode: 'browser' });
    return mockInvoke(command, args);
  }
};

// Environment info for debugging
export const getEnvironmentInfo = () => {
  const info = {
    isTauri: isTauriEnvironment(),
    userAgent: navigator.userAgent,
    platform: navigator.platform,
    timestamp: new Date().toISOString(),
    windowFeatures: {
      hasTauriIPC: typeof (window as any).__TAURI_IPC__ !== 'undefined',
      hasTauriAPI: typeof (window as any).__TAURI__ !== 'undefined'
    }
  };
  
  trail.light(990, { environment_info: info });
  return info;
};

// Store uploaded knowledge base with persistence
let uploadedKnowledge: any[] = [];

// Load persisted knowledge base on startup
if (typeof window !== 'undefined') {
  try {
    const stored = localStorage.getItem('voicecoach_knowledge_base');
    if (stored) {
      uploadedKnowledge = JSON.parse(stored);
      console.log(`üìö Loaded ${uploadedKnowledge.length} documents from storage`);
    }
  } catch (error) {
    console.warn('Failed to load stored knowledge base:', error);
  }
}

// Save knowledge base to localStorage
const saveKnowledgeBase = () => {
  if (typeof window !== 'undefined') {
    try {
      localStorage.setItem('voicecoach_knowledge_base', JSON.stringify(uploadedKnowledge));
      console.log(`üíæ Saved ${uploadedKnowledge.length} documents to storage`);
    } catch (error) {
      console.warn('Failed to save knowledge base:', error);
    }
  }
};

// Listen for document uploads
if (typeof window !== 'undefined') {
  window.addEventListener('documentUploaded', (event: Event) => {
    const customEvent = event as CustomEvent;
    const { filename, content, chunks } = customEvent.detail;
    
    // Check if document already exists (avoid duplicates)
    const existingIndex = uploadedKnowledge.findIndex(doc => doc.filename === filename);
    if (existingIndex >= 0) {
      // Update existing document
      uploadedKnowledge[existingIndex] = {
        filename,
        content,
        chunks,
        timestamp: Date.now()
      };
      console.log(`üìö Updated existing document: ${filename} (${chunks.length} chunks)`);
    } else {
      // Add new document
      uploadedKnowledge.push({
        filename,
        content,
        chunks,
        timestamp: Date.now()
      });
      console.log(`üìö Added new document: ${filename} (${chunks.length} chunks)`);
    }
    
    // Persist to localStorage
    saveKnowledgeBase();
  });
}

// Load core principles for filtering inappropriate suggestions
const loadCorePrinciples = async (): Promise<string> => {
  try {
    const response = await fetch('/docs/Core-Principles.md');
    if (response.ok) {
      return await response.text();
    }
  } catch (error) {
    console.warn('Could not load Core-Principles.md, using default filtering');
  }
  
  // Default fallback principles
  return `
PROHIBITED: Never suggest ending calls, hanging up, wrapping up conversations, or giving up on objections.
FOCUS: Keep conversations going, handle objections constructively, build value and trust, advance the sale.
BLOCK: end the call, hang up, wrap up, say goodbye, conclude, finish the call, close the conversation, terminate, disconnect
  `;
};

// Filter coaching suggestions against core principles
const filterCoachingSuggestion = (suggestion: string, _principles: string): { allowed: boolean; replacement?: string } => {
  const suggestionLower = suggestion.toLowerCase();
  // const principlesLower = principles.toLowerCase(); // Not used in this implementation
  
  // Check for prohibited phrases
  const blockedPhrases = [
    'end the call', 'hang up', 'wrap up', 'say goodbye', 'conclude the conversation',
    'finish the call', 'close the conversation', 'terminate', 'disconnect',
    'end on a high note', 'close positively'
  ];
  
  for (const phrase of blockedPhrases) {
    if (suggestionLower.includes(phrase)) {
      return {
        allowed: false,
        replacement: 'Ask deeper discovery questions to understand their specific needs and concerns'
      };
    }
  }
  
  return { allowed: true };
};

// Conversation context tracking for better coaching
let conversationHistory: string[] = [];
const MAX_CONTEXT_MESSAGES = 5; // Keep last 5 messages for context

// Add message to conversation context
const addToConversationContext = (text: string) => {
  conversationHistory.push(text);
  if (conversationHistory.length > MAX_CONTEXT_MESSAGES) {
    conversationHistory = conversationHistory.slice(-MAX_CONTEXT_MESSAGES);
  }
};

// Get conversation context for AI
const getConversationContext = () => {
  return conversationHistory.join(' ... ');
};

// Intelligent prompt compression to prevent Ollama truncation
interface PromptData {
  corePrinciples: string;
  relevantKnowledge: string;
  contextualExamples: string;
  conversationContextForPrompt: string;
  transcriptionText: string;
  detectedStage: string;
}

const buildOptimizedPrompt = (data: PromptData): string => {
  const OLLAMA_MAX_CHARS = 3800; // Leave buffer for 4096 token limit
  
  // Compress core principles to essential points only
  const compressedPrinciples = `NEVER suggest: ending calls, hanging up, wrapping up, giving up on objections.
ALWAYS: keep conversations going, handle objections constructively, advance the sale.`;

  // Use full conversation context (all 5 messages for maximum contextual awareness)
  const recentContext = data.conversationContextForPrompt;
  
  // Stage-specific knowledge filtering for proactive coaching
  const stageGuidance = getStageSpecificGuidance(data.detectedStage, data.relevantKnowledge);
  
  // Context-aware knowledge selection - prioritize most relevant chunks based on conversation
  const compressedKnowledge = compressKnowledgeBase(
    data.relevantKnowledge, 
    800, 
    {
      conversationContext: recentContext,
      currentStage: data.detectedStage,
      latestMessage: data.transcriptionText
    }
  );
  
  // Build stage-aware contextual examples
  const relevantExamples = getStageExamples(data.detectedStage, data.contextualExamples);
  
  const basePrompt = `Expert sales coach providing proactive guidance for ${data.detectedStage || 'ongoing'} stage conversation.

PRINCIPLES: ${compressedPrinciples}

STAGE GUIDANCE: ${stageGuidance}

KNOWLEDGE: ${compressedKnowledge || 'Apply relevant techniques from knowledge base'}

EXAMPLES: ${relevantExamples}

CONTEXT: ${recentContext}

LATEST: "${data.transcriptionText}"

Provide PROACTIVE next-step coaching (not reactive commentary). Focus on where to guide the conversation next.

JSON format:
{
  "urgency": "high|medium|low",
  "suggestion": "Proactive next step guidance (max 25 words)",
  "reasoning": "Why this advances the conversation (max 20 words)",
  "next_action": "Specific question or statement to say next (max 25 words)"
}`;

  // Always use full context prompt - monitor actual Ollama failures instead of pre-emptive cutting
  // If Ollama fails due to token limits, we'll catch and log it for optimization
  return basePrompt;
};

// Stage-specific guidance for proactive coaching
const getStageSpecificGuidance = (stage: string, _knowledge: string): string => {
  const stageGuides = {
    discovery: "Ask deeper questions to uncover needs, pain points, and decision criteria.",
    presentation: "Position solutions to specific needs mentioned. Build value and urgency.",
    objection: "Explore concerns deeper. Use 'That's exactly why...' technique.",
    closing: "Guide toward commitment. Ask for next steps or decision timeline.",
    unknown: "Identify current conversation stage through strategic questions."
  };
  
  return stageGuides[stage as keyof typeof stageGuides] || stageGuides.unknown;
};

// Enhanced knowledge selection with smart contextual relevance
const compressKnowledgeBase = (knowledge: string, maxChars: number, context?: {
  conversationContext?: string;
  currentStage?: string;
  latestMessage?: string;
}): string => {
  if (!knowledge || knowledge.length <= maxChars) return knowledge;
  
  // Enhanced knowledge selection: prioritize by relevance and technique importance
  return selectRelevantKnowledge(knowledge, maxChars, context);
};

// Smart knowledge selection based on context and conversation patterns
const selectRelevantKnowledge = (knowledge: string, maxChars: number, context?: {
  conversationContext?: string;
  currentStage?: string;
  latestMessage?: string;
}): string => {
  // Core negotiation techniques with contextual weighting
  const knowledgeCategories = {
    voss_core: {
      weight: 10,
      patterns: ["That's exactly why", "How am I supposed to", "calibrated question", "tactical empathy", "mirroring", "labeling"]
    },
    discovery_techniques: {
      weight: context?.currentStage === 'discovery' ? 12 : 8, // Boost if in discovery stage
      patterns: ["open-ended question", "pain point", "discovery", "tell me more", "help me understand"]
    },
    objection_handling: {
      weight: context?.currentStage === 'objection' ? 12 : 9, // Boost if handling objections
      patterns: ["objection", "concern", "but", "however", "pushback", "resistance"]
    },
    closing_strategies: {
      weight: context?.currentStage === 'closing' ? 12 : 7, // Boost if in closing stage
      patterns: ["close", "next steps", "decision", "commitment", "move forward", "ready"]
    },
    rapport_building: {
      weight: 6,
      patterns: ["rapport", "trust", "connection", "relationship", "empathy"]
    }
  };
  
  // Analyze conversation context for additional relevance signals
  const contextKeywords: string[] = [];
  if (context?.conversationContext) {
    const lowerContext = context.conversationContext.toLowerCase();
    if (lowerContext.includes('price') || lowerContext.includes('cost') || lowerContext.includes('budget')) {
      contextKeywords.push('pricing', 'budget', 'cost');
    }
    if (lowerContext.includes('timeline') || lowerContext.includes('when') || lowerContext.includes('deadline')) {
      contextKeywords.push('timeline', 'urgency', 'deadline');
    }
    if (lowerContext.includes('decision') || lowerContext.includes('approve') || lowerContext.includes('manager')) {
      contextKeywords.push('decision-maker', 'authority', 'approval');
    }
  }
  
  const sentences = knowledge.split('. ').filter(s => s.trim().length > 10);
  
  // Score and rank sentences by relevance
  const scoredSentences = sentences.map(sentence => {
    let score = 0;
    const lowerSentence = sentence.toLowerCase();
    
    // Calculate relevance score based on knowledge categories
    Object.entries(knowledgeCategories).forEach(([category, config]) => {
      const matches = config.patterns.filter(pattern => 
        lowerSentence.includes(pattern.toLowerCase())
      ).length;
      score += matches * config.weight;
    });
    
    // Bonus for practical examples and specific phrases
    if (lowerSentence.includes('example') || lowerSentence.includes('for instance')) score += 3;
    if (lowerSentence.includes('"') && lowerSentence.includes('say')) score += 4; // Direct quotes
    if (lowerSentence.length > 80 && lowerSentence.length < 200) score += 2; // Optimal length
    
    // Context-based relevance boost
    contextKeywords.forEach(keyword => {
      if (lowerSentence.includes(keyword)) score += 5;
    });
    
    return { sentence, score, length: sentence.length };
  });
  
  // Sort by relevance score (highest first)
  const rankedSentences = scoredSentences
    .filter(item => item.score > 0) // Only include relevant sentences
    .sort((a, b) => b.score - a.score);
  
  // Select sentences that fit within maxChars while maximizing value
  let selectedSentences = [];
  let totalLength = 0;
  
  for (const item of rankedSentences) {
    const potentialLength = totalLength + item.length + (selectedSentences.length > 0 ? 2 : 0); // +2 for '. '
    
    if (potentialLength <= maxChars) {
      selectedSentences.push(item.sentence);
      totalLength = potentialLength;
    }
    
    // Aim for 2-4 high-quality sentences rather than many low-quality ones
    if (selectedSentences.length >= 4) break;
  }
  
  // Fallback: if no relevant sentences found, use first sentences up to limit
  if (selectedSentences.length === 0) {
    let fallback = '';
    for (const sentence of sentences) {
      if (fallback.length + sentence.length + 2 <= maxChars) {
        fallback += (fallback ? '. ' : '') + sentence;
      } else {
        break;
      }
    }
    return fallback;
  }
  
  const result = selectedSentences.join('. ');
  return result.length > maxChars ? result.substring(0, maxChars - 3) + '...' : result;
};

// Get recent context (last N messages)
const getRecentContext = (fullContext: string, messageCount: number): string => {
  const messages = fullContext.split(' ... ');
  const recent = messages.slice(-messageCount);
  return recent.join(' ... ');
};

// Get stage-appropriate examples
const getStageExamples = (stage: string, examples: string): string => {
  if (!examples) return "";
  
  const stageKeywords = {
    discovery: ['understand', 'explore', 'tell me'],
    presentation: ['solution', 'recommend', 'offer'],
    objection: ['concern', 'but', 'however'],
    closing: ['ready', 'next steps', 'move forward']
  };
  
  const keywords = stageKeywords[stage as keyof typeof stageKeywords] || [];
  if (keywords.length === 0) return examples.substring(0, 200);
  
  // Find examples relevant to current stage
  const relevant = examples.split('\n').filter(line => 
    keywords.some(keyword => line.toLowerCase().includes(keyword))
  );
  
  return relevant.slice(0, 2).join('\n') || examples.substring(0, 200);
};

// Minimal prompt if compression fails
const buildMinimalPrompt = (data: PromptData): string => {
  return `Sales coach for ${data.detectedStage || 'conversation'}.

RULES: Never suggest ending calls. Always advance the sale.

LATEST: "${data.transcriptionText}"

Provide proactive next step:
{
  "urgency": "high|medium|low",
  "suggestion": "Next step guidance (max 20 words)",
  "reasoning": "Why this helps (max 15 words)",
  "next_action": "What to say next (max 20 words)"
}`;
};

// Ollama local AI integration for real-time coaching
export const generateOllamaCoaching = async (transcriptionText: string): Promise<any> => {
  // Start LED 100: Audio Input Processing
  const startTime = Date.now();
  trail.light(100, { 
    audio_input: 'transcription_received',
    text_length: transcriptionText.length,
    timestamp: new Date().toISOString(),
    is_empty: transcriptionText.trim().length === 0
  });

  // Add current message to context
  addToConversationContext(transcriptionText);
  
  // LED 200: Context Building Start
  const contextStart = Date.now();
  const conversationContext = getConversationContext();
  trail.light(200, { 
    context_building: 'conversation_history_assembly',
    context_length: conversationContext.length,
    model: 'qwen2.5:14b-instruct-q4_k_m',
    knowledge_docs_available: uploadedKnowledge.length
  });

  try {
    // LED 201: Core Principles Loading
    const corePrinciples = await loadCorePrinciples();
    trail.light(201, {
      core_principles: 'loaded',
      principles_length: corePrinciples.length
    });

    // LED 300: Knowledge Retrieval Start
    trail.light(300, {
      knowledge_retrieval: 'starting',
      total_docs: uploadedKnowledge.length,
      search_terms: transcriptionText.toLowerCase().split(' ').length
    });

    // Find relevant knowledge from uploaded documents with enhanced contextual matching
    let relevantKnowledge = '';
    let contextualExamples = '';
    let relevantChunksCount = 0;
    let enabledDocsCount = 0;
    let totalRelevanceScore = 0;
    
    if (uploadedKnowledge.length > 0) {
      const searchWords = transcriptionText.toLowerCase().split(' ');
      const conversationContextLower = conversationContext.toLowerCase();
      
      // LED 301: Document Filtering
      const enabledDocs = uploadedKnowledge.filter((_doc, index) => {
        const checkbox = document.querySelector(`#use-file-${index}`) as HTMLInputElement;
        return checkbox ? checkbox.checked : true; // Default to enabled if checkbox not found
      });
      enabledDocsCount = enabledDocs.length;
      
      trail.light(301, {
        document_filtering: 'complete',
        enabled_docs: enabledDocsCount,
        total_docs: uploadedKnowledge.length,
        filter_ratio: enabledDocsCount / uploadedKnowledge.length
      });
      
      console.log(`üìö Using ${enabledDocsCount} of ${uploadedKnowledge.length} documents for coaching`);
      
      // LED 302: Knowledge Processing Start
      trail.light(302, {
        knowledge_processing: 'chunk_analysis_start',
        enabled_docs: enabledDocsCount
      });

      for (const doc of enabledDocs) {
        // Look for contextual examples in the document
        if (doc.content.includes('contextual_examples')) {
          try {
            const docContent = JSON.parse(doc.content);
            if (docContent.contextual_examples) {
              // Find matching contextual examples based on conversation keywords
              for (const [topic, example] of Object.entries(docContent.contextual_examples)) {
                if (conversationContextLower.includes(topic.replace('_', ' ')) || 
                    transcriptionText.toLowerCase().includes(topic.replace('_', ' '))) {
                  contextualExamples += `\nCONTEXTUAL SUGGESTION for "${topic}": ${example}\n`;
                }
              }
            }
          } catch (e) {
            // Not JSON, continue with regular processing
            trail.light(303, {
              contextual_parsing: 'json_parse_failed',
              document: doc.filename,
              error: e instanceof Error ? e.message : 'Unknown error'
            });
          }
        }
        
        // Regular chunk processing for relevance
        for (const chunk of doc.chunks) {
          const chunkLower = chunk.toLowerCase();
          const relevanceScore = searchWords.filter(word => 
            word.length > 3 && chunkLower.includes(word)
          ).length;
          
          if (relevanceScore > 0) {
            relevantKnowledge += `\n[From ${doc.filename}]: ${chunk}\n`;
            relevantChunksCount++;
            totalRelevanceScore += relevanceScore;
          }
        }
      }

      // LED 304: Knowledge Processing Complete
      trail.light(304, {
        knowledge_processing: 'complete',
        relevant_chunks: relevantChunksCount,
        contextual_examples: contextualExamples.length,
        total_relevance_score: totalRelevanceScore,
        avg_relevance: relevantChunksCount > 0 ? totalRelevanceScore / relevantChunksCount : 0
      });
    } else {
      // LED 305: No Knowledge Base Available
      trail.light(305, {
        knowledge_processing: 'no_knowledge_base',
        fallback_mode: 'general_best_practices'
      });
    }

    // LED 402: Conversation Stage Detection (moved before prompt construction)
    const conversationContextForPrompt = getConversationContext();
    const stageKeywords = {
      discovery: ['tell me', 'explain', 'understand', 'how does', 'what is', 'help me understand'],
      presentation: ['solution', 'proposal', 'recommend', 'suggest', 'here\'s how', 'what we offer'],
      objection: ['but', 'however', 'concern', 'worried', 'not sure', 'problem with'],
      closing: ['ready', 'move forward', 'next steps', 'start', 'begin', 'sign up']
    };
    
    const detectedStage = Object.entries(stageKeywords).find(([_stage, keywords]) => 
      keywords.some(keyword => transcriptionText.toLowerCase().includes(keyword) || 
                             conversationContextForPrompt.toLowerCase().includes(keyword))
    )?.[0] || 'unknown';
    
    trail.light(402, {
      conversation_stage: 'detected',
      stage: detectedStage,
      context_analysis: 'complete',
      proactive_opportunity: detectedStage !== 'unknown' ? `stage_${detectedStage}_detected` : 'stage_unclear'
    });

    // LED 400: Prompt Construction Start
    const promptStart = Date.now();
    
    // Smart prompt compression to prevent Ollama truncation
    const compressedPrompt = buildOptimizedPrompt({
      corePrinciples,
      relevantKnowledge,
      contextualExamples,
      conversationContextForPrompt,
      transcriptionText,
      detectedStage
    });
    
    const prompt = compressedPrompt;

    // LED 401: Prompt Size Analysis (CRITICAL for truncation detection)
    const promptSizeBytes = new TextEncoder().encode(prompt).length;
    const promptSizeChars = prompt.length;
    const ollamaMaxTokens = 4096; // Typical Ollama context limit
    const estimatedTokens = Math.ceil(promptSizeChars / 4); // Rough token estimation
    
    // Calculate compression effectiveness
    const originalEstimate = Math.ceil((corePrinciples.length + relevantKnowledge.length + contextualExamples.length + conversationContextForPrompt.length + transcriptionText.length + 2000) / 4);
    const compressionRatio = originalEstimate > 0 ? (estimatedTokens / originalEstimate) : 1;
    const compressionEffective = estimatedTokens <= ollamaMaxTokens;
    
    trail.light(401, {
      prompt_construction: 'complete',
      prompt_size_chars: promptSizeChars,
      prompt_size_bytes: promptSizeBytes,
      estimated_tokens: estimatedTokens,
      original_estimated_tokens: originalEstimate,
      compression_ratio: compressionRatio,
      compression_effective: compressionEffective,
      ollama_max_tokens: ollamaMaxTokens,
      size_warning: estimatedTokens > ollamaMaxTokens ? 'FULL_CONTEXT_OVER_LIMIT' : 'FULL_CONTEXT_SUCCESS',
      truncation_risk: estimatedTokens > ollamaMaxTokens,
      full_context_preserved: true,
      context_messages_count: 5,
      proactive_coaching_enabled: detectedStage !== 'unknown',
      stage_detected: detectedStage,
      construction_time_ms: Date.now() - promptStart
    });
    
    // Log token limit warnings for monitoring
    if (estimatedTokens > ollamaMaxTokens) {
      console.warn(`üö® Token limit exceeded: ${estimatedTokens} > ${ollamaMaxTokens} (Full context preserved for better coaching)`);
    }
    
    // Dispatch token count event for dev mode overlay
    const tokenEvent = new CustomEvent('ollamaTokenCount', {
      detail: {
        estimatedTokens,
        promptSizeChars,
        isOverLimit: estimatedTokens > ollamaMaxTokens,
        timestamp: Date.now()
      }
    });
    window.dispatchEvent(tokenEvent);


    // LED 500: Ollama Request Start
    const requestStart = Date.now();
    trail.light(500, {
      ollama_request: 'starting',
      endpoint: 'http://localhost:11434/api/generate',
      model: 'qwen2.5:14b-instruct-q4_k_m',
      stream_mode: false,
      temperature: 0.3,
      max_tokens: 300
    });

    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'qwen2.5:14b-instruct-q4_k_m',
        prompt: prompt,
        stream: false,
        options: {
          temperature: 0.3,
          top_p: 0.9,
          num_predict: 300 // Increased for contextual suggestions
        }
      })
    }).catch(fetchError => {
      trail.fail(500, fetchError);
      throw new Error(`Ollama fetch failed: ${fetchError.message}`);
    });

    // LED 501: Ollama Response Timing
    const requestEndTime = Date.now();
    const requestDuration = requestEndTime - requestStart;
    
    if (!response.ok) {
      const errorText = await response.text().catch(() => 'Unknown error');
      const errorMessage = `Ollama request failed: ${response.status} ${response.statusText} - ${errorText}`;
      
      // Special handling for token limit errors
      if (response.status === 413 || response.status === 400) {
        console.error(`üö® POTENTIAL TOKEN LIMIT ISSUE: ${errorMessage}`);
        console.error(`üìä Prompt stats: ${promptSizeChars} chars, ~${estimatedTokens} tokens`);
        console.error(`üí° Consider reducing knowledge base or conversation context`);
      }
      
      trail.fail(500, new Error(errorMessage));
      throw new Error(errorMessage);
    }

    const data = await response.json().catch(jsonError => {
      trail.fail(501, jsonError);
      throw new Error(`Invalid JSON response from Ollama: ${jsonError.message}`);
    });
    const aiResponse = data.response;
    
    trail.light(501, {
      ollama_response: 'received',
      request_duration_ms: requestDuration,
      response_length: aiResponse?.length || 0,
      total_duration_ns: data.total_duration,
      load_duration_ns: data.load_duration,
      prompt_eval_count: data.prompt_eval_count,
      prompt_eval_duration_ns: data.prompt_eval_duration,
      eval_count: data.eval_count,
      eval_duration_ns: data.eval_duration,
      performance_warning: requestDuration > 5000 ? 'SLOW_RESPONSE' : 'normal_speed'
    });
    
    // LED 502: Response Processing Start
    // const processingStart = Date.now(); // Not used in current implementation
    trail.light(502, {
      response_processing: 'starting',
      raw_response_length: aiResponse?.length || 0,
      response_preview: aiResponse?.substring(0, 100) || 'empty_response'
    });

    // Parse JSON response from AI
    try {
      // LED 503: JSON Parsing
      const coaching = JSON.parse(aiResponse);
      trail.light(503, {
        json_parsing: 'success',
        coaching_fields: Object.keys(coaching),
        urgency: coaching.urgency,
        suggestion_preview: coaching.suggestion?.substring(0, 50) || 'no_suggestion'
      });
      
      // LED 504: Suggestion Quality Analysis
      const suggestionQuality = {
        has_specific_action: coaching.next_action?.length > 10,
        has_context: coaching.suggestion?.includes(transcriptionText.split(' ')[0]) || false,
        is_proactive: !coaching.suggestion?.toLowerCase().includes('continue') && 
                     !coaching.suggestion?.toLowerCase().includes('listen'),
        word_count: coaching.suggestion?.split(' ').length || 0
      };
      
      trail.light(504, {
        suggestion_quality: 'analyzed',
        quality_metrics: suggestionQuality,
        is_proactive: suggestionQuality.is_proactive,
        quality_score: Object.values(suggestionQuality).filter(Boolean).length / 4
      });

      // LED 505: Core Principles Filtering
      const filterStart = Date.now();
      const filterResult = filterCoachingSuggestion(coaching.suggestion, corePrinciples);
      
      trail.light(505, {
        core_principles_filter: 'complete',
        filter_allowed: filterResult.allowed,
        filter_duration_ms: Date.now() - filterStart,
        original_suggestion_length: coaching.suggestion?.length || 0
      });
      
      if (!filterResult.allowed) {
        // LED 506: Suggestion Replacement
        coaching.suggestion = filterResult.replacement || 'Ask discovery questions to understand their needs better';
        coaching.reasoning = 'Filtered inappropriate suggestion - keeping conversation focused on sales objectives';
        trail.light(506, { 
          suggestion_replacement: 'applied',
          original_suggestion: coaching.suggestion,
          replacement: filterResult.replacement,
          reason: 'core_principles_violation'
        });
      }

      // LED 600: Final Coaching Output
      const totalDuration = Date.now() - startTime;
      const finalCoaching = {
        type: 'ollama_coaching',
        urgency: coaching.urgency,
        suggestion: coaching.suggestion,
        reasoning: coaching.reasoning,
        next_action: coaching.next_action,
        model: 'qwen2.5:14b',
        latency_ms: data.total_duration ? Math.round(data.total_duration / 1000000) : 0
      };
      
      trail.light(600, { 
        coaching_generation: 'complete',
        urgency: coaching.urgency,
        suggestion_length: coaching.suggestion?.length || 0,
        filtered: !filterResult.allowed,
        total_pipeline_duration_ms: totalDuration,
        context_building_time: contextStart ? Date.now() - contextStart : 0,
        knowledge_chunks_used: relevantChunksCount,
        suggestion_type: suggestionQuality.is_proactive ? 'proactive' : 'reactive',
        stage_detected: detectedStage,
        final_output: 'ready_for_ui'
      });
      
      return finalCoaching;
      
    } catch (parseError) {
      // LED 507: JSON Parse Failure
      trail.light(507, { 
        json_parse_error: 'failed',
        raw_response: aiResponse?.substring(0, 200) || 'no_response',
        error_message: parseError instanceof Error ? parseError.message : 'unknown_parse_error',
        fallback_mode: 'plain_text_processing'
      });
      
      // LED 508: Fallback Processing
      const fallbackSuggestion = aiResponse?.split('\n')[0] || 'Continue the conversation';
      trail.light(508, {
        fallback_processing: 'complete',
        fallback_suggestion: fallbackSuggestion.substring(0, 50),
        total_duration_ms: Date.now() - startTime
      });
      
      return {
        type: 'ollama_coaching',
        urgency: 'medium',
        suggestion: fallbackSuggestion,
        reasoning: 'AI coaching analysis (fallback mode)',
        next_action: 'Listen actively and respond',
        model: 'qwen2.5:14b',
        latency_ms: data.total_duration ? Math.round(data.total_duration / 1000000) : 0
      };
    }
  } catch (error) {
    // LED 100 failure: Complete Pipeline Failure
    const errorDetails = {
      error_type: error instanceof Error ? error.constructor.name : 'unknown',
      error_message: error instanceof Error ? error.message : 'unknown_error',
      error_stack: error instanceof Error ? error.stack?.substring(0, 200) : 'no_stack',
      pipeline_stage: 'unknown',
      total_duration_ms: Date.now() - startTime
    };
    
    trail.fail(100, error as Error);
    console.error('Ollama coaching pipeline failed:', errorDetails);
    return null;
  }
};

// Export function to get stored knowledge base
export const getStoredKnowledgeBase = () => {
  return uploadedKnowledge;
};

// Export function to clear knowledge base
export const clearKnowledgeBase = () => {
  uploadedKnowledge = [];
  if (typeof window !== 'undefined') {
    localStorage.removeItem('voicecoach_knowledge_base');
    console.log('üóëÔ∏è Cleared knowledge base');
  }
};

// Video Platform Detection for Enhanced Coaching
const detectVideoPlatform = (): void => {
  try {
    // Detect video platforms based on window title, URL patterns, etc.
    const userAgent = navigator.userAgent.toLowerCase();
    const currentUrl = window.location.href.toLowerCase();
    const windowTitle = document.title.toLowerCase();
    
    // Check for Zoom
    if (currentUrl.includes('zoom.us') || windowTitle.includes('zoom') || 
        document.querySelector('[data-zoom]') || userAgent.includes('zoom')) {
      // LED 171: Zoom integration detected
      trail.light(171, {
        zoom_integration_detected: true,
        platform: 'zoom',
        url_detected: currentUrl.includes('zoom.us'),
        title_detected: windowTitle.includes('zoom')
      });
      
      // LED 176: Video call coaching activated
      trail.light(176, {
        video_call_coaching_activated: true,
        platform: 'zoom',
        coaching_mode: 'real_time'
      });
    }
    
    // Check for Microsoft Teams
    if (currentUrl.includes('teams.microsoft.com') || windowTitle.includes('teams') ||
        document.querySelector('[data-teams]') || userAgent.includes('teams')) {
      // LED 172: Teams integration detected
      trail.light(172, {
        teams_integration_detected: true,
        platform: 'microsoft_teams',
        url_detected: currentUrl.includes('teams.microsoft.com'),
        title_detected: windowTitle.includes('teams')
      });
      
      // LED 176: Video call coaching activated
      trail.light(176, {
        video_call_coaching_activated: true,
        platform: 'microsoft_teams',
        coaching_mode: 'real_time'
      });
    }
    
    // Check for Google Meet
    if (currentUrl.includes('meet.google.com') || windowTitle.includes('meet') ||
        document.querySelector('[data-meet]') || userAgent.includes('hangouts')) {
      // LED 173: Meet integration detected
      trail.light(173, {
        meet_integration_detected: true,
        platform: 'google_meet',
        url_detected: currentUrl.includes('meet.google.com'),
        title_detected: windowTitle.includes('meet')
      });
      
      // LED 176: Video call coaching activated
      trail.light(176, {
        video_call_coaching_activated: true,
        platform: 'google_meet',
        coaching_mode: 'real_time'
      });
    }
    
    // LED 177: Video platform audio quality check
    trail.light(177, {
      video_platform_audio_quality_check: true,
      audio_context_available: typeof AudioContext !== 'undefined',
      media_devices_available: !!navigator.mediaDevices,
      display_media_supported: !!navigator.mediaDevices?.getDisplayMedia
    });
    
    // LED 179: Video platform integration complete
    trail.light(179, {
      video_platform_integration_complete: true,
      detection_complete: true,
      coaching_ready: true
    });
    
  } catch (error) {
    console.warn('Video platform detection failed:', error);
    trail.fail(170, error as Error);
  }
};

// Add debug commands to window for testing
if (typeof window !== 'undefined') {
  (window as any).voicecoachDebug = {
    getEnvironmentInfo,
    isTauriEnvironment,
    testMockAPI: (command: string, args?: any) => mockInvoke(command, args),
    smartInvoke,
    generateOllamaCoaching, // Add Ollama testing function
    getStoredKnowledgeBase, // Debug: check stored docs
    clearKnowledgeBase, // Debug: clear all docs
    
    // System audio debug functions
    startSystemAudio: (mode: string = 'system_audio') => startSystemAudioCapture(mode, 'default'),
    stopSystemAudio: stopSystemAudioCapture,
    getSystemAudioStatus: () => ({
      isActive: isSystemAudioActive,
      currentMode: currentAudioMode,
      hasStream: !!systemAudioStream,
      hasRecorder: !!systemAudioRecorder
    }),
    testAudioModes: async () => {
      console.log('üß™ Testing all audio modes...');
      for (const mode of ['microphone_only', 'system_audio', 'combined']) {
        try {
          const result = await mockInvoke('set_audio_mode', { mode });
          console.log(`‚úÖ ${mode}: ${result}`);
        } catch (error) {
          console.log(`‚ùå ${mode}: ${error}`);
        }
      }
    },
    
    // Enhanced audio debug functions
    detectVideoPlatform,
    getEnhancedAudioStats: () => {
      if (typeof window !== 'undefined' && window.debug?.breadcrumbs?.getEnhancedAudioStats) {
        return window.debug.breadcrumbs.getEnhancedAudioStats();
      }
      return { message: 'Enhanced audio stats not available' };
    },
    testVideoCallFlow: async () => {
      console.log('üé• Testing complete video call flow...');
      try {
        // Test video platform detection
        detectVideoPlatform();
        
        // Test system audio capture
        await startSystemAudioCapture('system_audio', 'default');
        
        // Test enhanced coaching pipeline
        await processSystemAudioChunk(new Blob(['test'], { type: 'audio/webm' }));
        
        console.log('‚úÖ Video call flow test complete');
      } catch (error) {
        console.log('‚ùå Video call flow test failed:', error);
      }
    }
  };
}